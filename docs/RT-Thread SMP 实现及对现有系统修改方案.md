# RT-Thread SMP 实现及对现有系统修改方案

## 1 多处理器平台和单处理器平台的差异：

##### 在多处理器平台上，由于包含多CPU的并行运行，在概念上相较于单处理器系统有了不少的增加，而且原先基于单处理器系统设计的一些排它性假设也已经不再成立，以下对这些相关内容进行了阐述：

1 单处理器系统在禁止中断响应时，所进行的任何操作都是独占的，所以单处理器系统中对临界区的保护往往是这样：

```
关中断
进行临界区数据读取、计算、更新操作
恢复中断
```

当前版本的RT-Thread遵循了这一序列。

然而，在多处理器系统中，即使是当前CPU处理中断禁止状态，其他的CPU仍然可能会进行临界区的访问。

多处理器系统中引入了自旋锁来解决多处理器间的并发问题。自旋器使多处理器在访问临界区时，同时只会有一个在争锁竞争中获得锁，从而排它性进入临界区。

2 单处理器系统中，只有当前任务是“活跃”的，即只有一个任务占有CPU资源，其他的任务状态无论是否是READY，都是处于等待状态。而在多处理器系统中，每个CPU上都会有一个活跃的当前任务，这给进程管理的处理上会带来一些差别。

比如单处理器系统上可以suspend任何一个READY状态的任务，这时非当前任务已经是等待状态; 而当前任务由于在schedule点上，实质上也是处于暂停状态，只需做好状态保存即可。suspend执行任务的状态改变并进行任务调度后，被schedule的任务就可以确保不再被执行下去。而在多处理系统中，suspend要操作的目标任务却有可能是其他CPU的当前任务，更有可能它还关闭了中断响应，对于这种情况的出现，如何去suspend另一个任务需要一些策略上的讨论，由于这种情况目前较为少见，最初版的设计为不支持suspend、delete对这种其他CPU当前任务的操作。

3 各CPU间的通讯

多处理器的加入也增加了一些额外的需求，比如多CPU的启动过程和CPU间的事件通知。

主CPU的启动大体上和单处理器没太多区别，只是增加了对锁的初始化和对其他CPU的唤醒。这里的唤醒方式因每个平台而异。主CPU在对全局硬件、自身和系统初始化完成后，会唤醒其他的CPU，其他的CPU则只完成自身相关的初始化后，就开始了任务调度过程。

CPU间的通讯是通过核间中断完成的。在qemu及相当一部分armv7平台上，源CPU通过GIC向目标CPU发送SGI中断，对于这些中断，目标CPU会接收到此类IPI中断后会进行相应处理，从而实现通知功能。



## 2 多处理系统内核实现：

引入了一个大内核锁 rt_kernel_lock，称之为大，是因为对于任何的共享变量访问，都需要锁定此锁，以求得最大程度的安全性。大内核锁的方式对于多处理器系统来说，由于粒度太粗会降低系统的总性能，但第一版的目标为让RT-Thread在SMP上正常运行，这可以在后续版本的优化过程中把它拆分，按所要操作的对象细分成一个一个的细粒度锁。

以下当前任务的概念为当前CPU的per_cpu变量

### 1 锁定rt_kernel_lock的实现：

```
如果是当前任务的lock计数为0，则
    禁止本地中断
    当前任务preemt计数 ++
    spin_lock(rt_kernel_lock)
当前任务的lock计数 ++
    
```

由于在获取了自旋锁后的任务不宜被切换走，所以这里要加上对preemt的自增操作。

## 2 解锁rt_kernel_lock的实现：

```
当前任务的lock计数 --
如果当前任务的lock计数为0,则
    spin_unlock(rt_kernel_lock)
    当前任务preemt计数 --	
    恢复本地中断
```



### 3 任务切换函数的实现：

中间插入解锁操作的原因是因为因中断而被抢占的任务以及新任务不会有后续的解锁动作。

这里解锁是安全的，因为这个时刻，已经不再会使用旧任务的任何信息。


```
锁定rt_kernel_lock
若旧任务不为空，则进行状态保存
切换到新任务的sp
判断新任务的lock计数，若为0，为其做一次spin_unlock(rt_kernel_lock)
恢复新任务的状态
解锁rt_kernel_lock
```

下表说明了preemt计数和lock计数在各种情况下的可能值：

|                              | preemt计数 | lock计数 |
| ---------------------------- | ---------- | :------: |
| 新任务                       | 0          |    0     |
| 主动调任务切换而被切走的任务 | 1          |    1     |
| 被中断抢占的任务             | 任何值 [a] |    0     |

注a: 被中断的任务可能已经调用过增加preemt计数的函数



### 4 系统初始化的实现：

在进行任务切换时，都是要锁定rt_kernel_lock的，所以系统的初始化（包括各CPU的第一次任务运行切换）的相关流程的实现形式:

```
初始化...
锁定rt_kernel_lock
切换到第一个就绪的任务
```



### 5 任务框架如下面的形式实现：

```
任务的初始化代码
任务的工作循环：
   功能流程
   任务切换函数
   功能流程
```



### 6 关于中断的处理：

执行在中断处理函数中时，肯定没有锁定rt_kernel_lock，所以中断处理函数中的相关部分形式如下:

```
spin_lock(rt_kernel_lock)
执行中断处理程序
若切换标志为0，则说明没有切换需求：
	spin_unlock(rt_kernel_lock)
	恢复被中断现场状态
    中断返回到原来任务
否则：
    若旧任务不为空，则进行状态保存
    切换到新任务的sp
    判断新任务的lock计数，若为0，为其做一次spin_unlock(rt_kernel_lock)
    恢复新任务的状态
    中断返回到新任务
```



### 综上所述，这里的实现原理在于几点：

1  由于大内核锁的引入，把内核看成一个大的临界区，所有需要对内核进行操作的时候，都需要锁定内核锁。

2 由于调度的过程比较特殊，它需要“跨任务”解锁，在此过程中可能出现目标任务是未锁定大内核锁的，也就说目录任务并不是主动调用的切换函数，这时需要切换器给其补上解锁的操作。

 需要补解锁的本质原因，是在于对于新任务和被中断的任务来说，它们不会在返回任务后执行解锁操作。

3 大内核锁解锁的时机，由于大内核锁一旦被解锁，其他的CPU就有可能立即进入到临界区,所以必须在确保其他CPU不会有误解的情况出现。

实例：CPU0的当前任务take一个信号量，于是被挂入到某信号量的等待队列，此时CPU0上尚未执行完选取新任务切换的步骤，其实当前任务仍然在CPU0上运行，它的任务上下文尚未保存到任务栈中去，而CPU1此时已经执行到了此信号量的release操作，但由于spinlock的作用，它在等待访问信号量的等待队列，在此过程中如果过早开锁，则CPU1很可能重新调度了CPU0目前的当前任务，出现了很严重的并发错误。



## 3 RT-Thread的修改涉及点

### 1 spin_lock实现

rt_hw_interrupt_xxx两个函数的，由于rt_hw_interrupt_xxx是嵌套的配对使用，所以这两个函数实现成对于当前任务只在第一次获取锁时才真正去做spin_lock动作，最后一次释放才真正解锁大内核锁。

<u>这个部分先在单核版本模拟加入，涉及到任务结构体变量和中断的修改及调试大约需要两天的时间</u>



### 2 调度器及相关修改

这里涉及到的范围比较广，涉及到全局变量的结构修改、调度器对最高优先级的选择及相关的suspend和resume等函数的目标队列判定

<u>这个部分先在单核版本模拟加入，以多核的代码先在单核版本上跑通，大约需要2～3天时间</u>



### 3 多核启动代码加入

这里最初始支持为双核，这一部分的代码是和平台构架相依赖的。

这部分会涉及到GIC的处理、MMU开启等内容。

<u>这部分会很快完成，在已验证平台可以忽略不计</u>



### 4 多核运行的调试及代码调整

这部分工作比较琐碎，主要是查找多核间运行问题的步骤，编码量不大，但需具体分析出现的问题，可能要对原有系统代码进行调整。

现在可以预见的问题是，目前的RT-Thread所有主动切换代码中，rt_schedule都是在rt_hw_interrupt_enable的最后一层嵌套退出之后再去调用。而在多核系统中rt_hw_interrupt_enable最后一层退出意味着释放大内核锁，这样可能导致另外的CPU提前调度了当前CPU的尚未完全退出的当前任务，此类的问题的修改往往需要批量调整原有系统的代码结构。

<u>此类问题的全部解决时间上的预估不太准确，但应该预留3～4天的时间去做调试工作。</u>



### 5 加入对核间中断的响应

<u>这个部分涉及范围不大，但由于是中断的处理，可能更多是测试更占时间一些，预估一天可以完成。</u>



至此多核系统的最初版本应该能运行，留几天的综合调试时间。

<u>综上所述，大约15天内可以完成最初版本的修改工作。</u>